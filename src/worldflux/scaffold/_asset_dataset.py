"""Scaffold template asset. Generated from sample-dreamer."""

DATASET_PY = 'from __future__ import annotations\n\nimport time\nimport warnings\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\n\nfrom worldflux.training import ReplayBuffer\nfrom worldflux.training.data import create_random_buffer\n\ntry:\n    import tomllib  # Python 3.11+\nexcept ModuleNotFoundError:  # pragma: no cover\n    import tomli as tomllib\n\nPhaseCallback = Callable[[str, str | None], None]\nFrameCallback = Callable[[Any, int, int, float, bool], None]\nCleanupFn = Callable[[], None]\n\n\ndef _emit_phase(callback: PhaseCallback | None, phase: str, detail: str | None = None) -> None:\n    if callback is None:\n        return\n    try:\n        callback(phase, detail)\n    except Exception:\n        # Dashboard callbacks should never interrupt training data creation.\n        return\n\n\ndef _emit_frame(\n    callback: FrameCallback | None,\n    frame: Any,\n    episode: int,\n    episode_step: int,\n    reward: float,\n    done: bool,\n) -> None:\n    if callback is None:\n        return\n    try:\n        callback(frame, episode, episode_step, reward, done)\n    except Exception:\n        # Dashboard callbacks should never interrupt training data creation.\n        return\n\n\ndef _load_project_config(path: str = "worldflux.toml") -> dict[str, Any]:\n    with Path(path).open("rb") as f:\n        return tomllib.load(f)\n\n\ndef _parse_obs_shape(value: Any) -> tuple[int, ...]:\n    if isinstance(value, list | tuple) and value:\n        return tuple(int(dim) for dim in value)\n    raise ValueError(f"Invalid obs_shape: {value!r}")\n\n\ndef _fit_vector(value: Any, dim: int) -> np.ndarray:\n    flat = np.asarray(value, dtype=np.float32).reshape(-1)\n    out = np.zeros(dim, dtype=np.float32)\n    size = min(dim, flat.size)\n    out[:size] = flat[:size]\n    return out\n\n\ndef _fit_visual_obs(obs: Any, target_shape: tuple[int, ...]) -> np.ndarray:\n    target_c, target_h, target_w = target_shape\n    arr = np.asarray(obs)\n    if arr.ndim == 2:\n        arr = np.expand_dims(arr, axis=-1)\n    if arr.ndim == 3 and arr.shape[0] not in (1, 3, 4):\n        arr = arr.transpose(2, 0, 1)\n    if arr.ndim != 3:\n        arr = np.resize(arr, target_shape)\n\n    if arr.shape[0] < target_c:\n        pad = np.zeros((target_c - arr.shape[0], arr.shape[1], arr.shape[2]), dtype=arr.dtype)\n        arr = np.concatenate([arr, pad], axis=0)\n    arr = arr[:target_c]\n    if arr.shape[1:] != (target_h, target_w):\n        arr = np.resize(arr, target_shape)\n\n    arr = arr.astype(np.float32)\n    if np.max(arr) > 1.0:\n        arr /= 255.0\n    return arr\n\n\ndef _one_hot(action: int, dim: int) -> np.ndarray:\n    vec = np.zeros(dim, dtype=np.float32)\n    vec[action % dim] = 1.0\n    return vec\n\n\nclass OnlineAtariBatchProvider:\n    """BatchProvider that alternates online Atari rollout and replay sampling."""\n\n    def __init__(\n        self,\n        *,\n        gym_env: str,\n        obs_shape: tuple[int, ...],\n        action_dim: int,\n        capacity: int,\n        warmup_transitions: int,\n        collect_steps_per_update: int,\n        max_episode_steps: int,\n        frame_callback: FrameCallback | None = None,\n        phase_callback: PhaseCallback | None = None,\n        frame_fps: int = 8,\n    ):\n        self._obs_shape = obs_shape\n        self._action_dim = action_dim\n        self._collect_steps_per_update = max(1, int(collect_steps_per_update))\n        self._warmup_transitions = max(1, int(warmup_transitions))\n        self._max_episode_steps = max(8, int(max_episode_steps))\n        self._frame_callback = frame_callback\n        self._phase_callback = phase_callback\n\n        self._buffer = ReplayBuffer(capacity=capacity, obs_shape=obs_shape, action_dim=action_dim)\n        self._episode_obs: list[np.ndarray] = []\n        self._episode_actions: list[np.ndarray] = []\n        self._episode_rewards: list[float] = []\n        self._episode_dones: list[float] = []\n        self._episode_step = 0\n        self._episode_index = 0\n\n        self._min_frame_interval = 1.0 / max(1, int(frame_fps))\n        self._last_frame_time = 0.0\n        self._closed = False\n\n        try:\n            import ale_py\n            import gymnasium as gym\n\n            gym.register_envs(ale_py)\n            self._env = gym.make(gym_env, render_mode=None)\n        except Exception as exc:  # pragma: no cover - depends on optional extras\n            raise RuntimeError(\n                "Install gymnasium + ale-py to enable live gameplay. "\n                f"(failed to initialize Atari env \'{gym_env}\': {exc})"\n            ) from exc\n\n        self._obs, _ = self._env.reset()\n        _emit_phase(self._phase_callback, "collecting")\n        self._collect_until(self._warmup_transitions)\n\n    def _flush_episode(self) -> None:\n        if not self._episode_obs:\n            return\n        self._buffer.add_episode(\n            obs=np.asarray(self._episode_obs, dtype=np.float32),\n            actions=np.asarray(self._episode_actions, dtype=np.float32),\n            rewards=np.asarray(self._episode_rewards, dtype=np.float32),\n            dones=np.asarray(self._episode_dones, dtype=np.float32),\n        )\n        self._episode_obs.clear()\n        self._episode_actions.clear()\n        self._episode_rewards.clear()\n        self._episode_dones.clear()\n\n    def _step_once(self) -> None:\n        action = int(self._env.action_space.sample())\n        next_obs, reward, terminated, truncated, _ = self._env.step(action)\n        done = bool(terminated or truncated)\n\n        self._episode_obs.append(_fit_visual_obs(self._obs, self._obs_shape))\n        self._episode_actions.append(_one_hot(action, self._action_dim))\n        self._episode_rewards.append(float(reward))\n        self._episode_dones.append(float(done))\n\n        self._episode_step += 1\n\n        now = time.monotonic()\n        if now - self._last_frame_time >= self._min_frame_interval or done:\n            _emit_frame(\n                self._frame_callback,\n                next_obs,\n                self._episode_index + 1,\n                self._episode_step,\n                float(reward),\n                done,\n            )\n            self._last_frame_time = now\n\n        boundary = done or self._episode_step >= self._max_episode_steps\n        if boundary:\n            self._flush_episode()\n            self._episode_index += 1\n            self._episode_step = 0\n            self._obs, _ = self._env.reset()\n        else:\n            self._obs = next_obs\n\n    def _collect_steps(self, steps: int) -> None:\n        for _ in range(max(0, int(steps))):\n            self._step_once()\n\n    def _collect_until(self, min_transitions: int) -> None:\n        target = max(1, int(min_transitions))\n        guard = 0\n        max_guard = max(target * 8, self._max_episode_steps * 8, 2048)\n        while len(self._buffer) < target and guard < max_guard:\n            self._step_once()\n            guard += 1\n\n        if len(self._buffer) < target:\n            if self._episode_obs:\n                self._flush_episode()\n            if len(self._buffer) < target:\n                raise RuntimeError(\n                    f"Unable to collect enough online transitions: have {len(self._buffer)}, need {target}."\n                )\n\n    def sample(self, batch_size: int, seq_len: int, device: str = "cpu"):\n        if self._closed:\n            raise RuntimeError("OnlineAtariBatchProvider is already closed")\n\n        needed = max(int(seq_len), 1)\n        ready_target = max(self._warmup_transitions, needed)\n        if len(self._buffer) < ready_target:\n            self._collect_until(ready_target)\n\n        self._collect_steps(self._collect_steps_per_update)\n\n        if len(self._buffer) < needed:\n            self._collect_until(needed)\n\n        return self._buffer.sample(batch_size=batch_size, seq_len=seq_len, device=device)\n\n    def close(self) -> None:\n        if self._closed:\n            return\n        self._closed = True\n        try:\n            self._env.close()\n        except Exception:\n            return\n\n\nclass OnlineMujocoBatchProvider:\n    """BatchProvider that alternates online MuJoCo rollout and replay sampling."""\n\n    def __init__(\n        self,\n        *,\n        gym_env: str,\n        obs_shape: tuple[int, ...],\n        action_dim: int,\n        capacity: int,\n        warmup_transitions: int,\n        collect_steps_per_update: int,\n        max_episode_steps: int,\n        frame_callback: FrameCallback | None = None,\n        phase_callback: PhaseCallback | None = None,\n        frame_fps: int = 8,\n    ):\n        self._obs_dim = int(obs_shape[0])\n        self._action_dim = action_dim\n        self._collect_steps_per_update = max(1, int(collect_steps_per_update))\n        self._warmup_transitions = max(1, int(warmup_transitions))\n        self._max_episode_steps = max(8, int(max_episode_steps))\n        self._frame_callback = frame_callback\n        self._phase_callback = phase_callback\n\n        self._buffer = ReplayBuffer(capacity=capacity, obs_shape=(self._obs_dim,), action_dim=action_dim)\n        self._episode_obs: list[np.ndarray] = []\n        self._episode_actions: list[np.ndarray] = []\n        self._episode_rewards: list[float] = []\n        self._episode_dones: list[float] = []\n        self._episode_step = 0\n        self._episode_index = 0\n\n        self._min_frame_interval = 1.0 / max(1, int(frame_fps))\n        self._last_frame_time = 0.0\n        self._closed = False\n\n        try:\n            import gymnasium as gym\n\n            self._env = gym.make(gym_env, render_mode="rgb_array")\n        except Exception as exc:  # pragma: no cover - depends on optional extras\n            raise RuntimeError(\n                "Install gymnasium + mujoco to enable live gameplay. "\n                f"(failed to initialize MuJoCo env \'{gym_env}\': {exc})"\n            ) from exc\n\n        self._obs, _ = self._env.reset()\n        _emit_phase(self._phase_callback, "collecting")\n        self._collect_until(self._warmup_transitions)\n\n    def _flush_episode(self) -> None:\n        if not self._episode_obs:\n            return\n        self._buffer.add_episode(\n            obs=np.asarray(self._episode_obs, dtype=np.float32),\n            actions=np.asarray(self._episode_actions, dtype=np.float32),\n            rewards=np.asarray(self._episode_rewards, dtype=np.float32),\n            dones=np.asarray(self._episode_dones, dtype=np.float32),\n        )\n        self._episode_obs.clear()\n        self._episode_actions.clear()\n        self._episode_rewards.clear()\n        self._episode_dones.clear()\n\n    def _step_once(self) -> None:\n        action = self._env.action_space.sample()\n        next_obs, reward, terminated, truncated, _ = self._env.step(action)\n        done = bool(terminated or truncated)\n\n        self._episode_obs.append(_fit_vector(self._obs, self._obs_dim))\n        self._episode_actions.append(_fit_vector(action, self._action_dim))\n        self._episode_rewards.append(float(reward))\n        self._episode_dones.append(float(done))\n\n        self._episode_step += 1\n\n        now = time.monotonic()\n        if now - self._last_frame_time >= self._min_frame_interval or done:\n            frame = self._env.render()\n            _emit_frame(\n                self._frame_callback,\n                frame,\n                self._episode_index + 1,\n                self._episode_step,\n                float(reward),\n                done,\n            )\n            self._last_frame_time = now\n\n        boundary = done or self._episode_step >= self._max_episode_steps\n        if boundary:\n            self._flush_episode()\n            self._episode_index += 1\n            self._episode_step = 0\n            self._obs, _ = self._env.reset()\n        else:\n            self._obs = next_obs\n\n    def _collect_steps(self, steps: int) -> None:\n        for _ in range(max(0, int(steps))):\n            self._step_once()\n\n    def _collect_until(self, min_transitions: int) -> None:\n        target = max(1, int(min_transitions))\n        guard = 0\n        max_guard = max(target * 8, self._max_episode_steps * 8, 2048)\n        while len(self._buffer) < target and guard < max_guard:\n            self._step_once()\n            guard += 1\n\n        if len(self._buffer) < target:\n            if self._episode_obs:\n                self._flush_episode()\n            if len(self._buffer) < target:\n                raise RuntimeError(\n                    f"Unable to collect enough online transitions: have {len(self._buffer)}, need {target}."\n                )\n\n    def sample(self, batch_size: int, seq_len: int, device: str = "cpu"):\n        if self._closed:\n            raise RuntimeError("OnlineMujocoBatchProvider is already closed")\n\n        needed = max(int(seq_len), 1)\n        ready_target = max(self._warmup_transitions, needed)\n        if len(self._buffer) < ready_target:\n            self._collect_until(ready_target)\n\n        self._collect_steps(self._collect_steps_per_update)\n\n        if len(self._buffer) < needed:\n            self._collect_until(needed)\n\n        return self._buffer.sample(batch_size=batch_size, seq_len=seq_len, device=device)\n\n    def close(self) -> None:\n        if self._closed:\n            return\n        self._closed = True\n        try:\n            self._env.close()\n        except Exception:\n            return\n\n\ndef _collect_atari_buffer(\n    *,\n    gym_env: str,\n    obs_shape: tuple[int, ...],\n    action_dim: int,\n    num_episodes: int,\n    episode_length: int,\n    capacity: int,\n    frame_callback: FrameCallback | None = None,\n    phase_callback: PhaseCallback | None = None,\n    frame_fps: int = 8,\n) -> ReplayBuffer | None:\n    try:\n        import ale_py\n        import gymnasium as gym\n\n        gym.register_envs(ale_py)\n    except Exception as exc:  # pragma: no cover - depends on optional extras\n        warnings.warn(f"Atari gym collection is unavailable: {exc}")\n        _emit_phase(\n            phase_callback,\n            "unavailable",\n            "Install gymnasium + ale-py to enable live gameplay.",\n        )\n        return None\n\n    try:\n        env = gym.make(gym_env, render_mode=None)\n    except Exception as exc:  # pragma: no cover - depends on optional extras\n        warnings.warn(f"Failed to create Atari env \'{gym_env}\': {exc}")\n        _emit_phase(phase_callback, "unavailable", f"Failed to create Atari env: {gym_env}")\n        return None\n\n    _emit_phase(phase_callback, "collecting")\n    buffer = ReplayBuffer(capacity=capacity, obs_shape=obs_shape, action_dim=action_dim)\n    min_frame_interval = 1.0 / max(1, int(frame_fps))\n    last_frame_time = 0.0\n\n    for episode_idx in range(num_episodes):\n        obs, _ = env.reset()\n        ep_obs: list[np.ndarray] = []\n        ep_actions: list[np.ndarray] = []\n        ep_rewards: list[float] = []\n        ep_dones: list[float] = []\n        done = False\n        steps = 0\n\n        while not done and steps < episode_length:\n            action = int(env.action_space.sample())\n            next_obs, reward, terminated, truncated, _ = env.step(action)\n            done = bool(terminated or truncated)\n            ep_obs.append(_fit_visual_obs(obs, obs_shape))\n            ep_actions.append(_one_hot(action, action_dim))\n            ep_rewards.append(float(reward))\n            ep_dones.append(float(done))\n            now = time.monotonic()\n            if now - last_frame_time >= min_frame_interval or done:\n                _emit_frame(\n                    frame_callback,\n                    next_obs,\n                    episode_idx + 1,\n                    steps + 1,\n                    float(reward),\n                    done,\n                )\n                last_frame_time = now\n            obs = next_obs\n            steps += 1\n\n        if ep_obs:\n            buffer.add_episode(\n                obs=np.asarray(ep_obs, dtype=np.float32),\n                actions=np.asarray(ep_actions, dtype=np.float32),\n                rewards=np.asarray(ep_rewards, dtype=np.float32),\n                dones=np.asarray(ep_dones, dtype=np.float32),\n            )\n    env.close()\n    return buffer if len(buffer) > 0 else None\n\n\ndef _collect_mujoco_buffer(\n    *,\n    gym_env: str,\n    obs_shape: tuple[int, ...],\n    action_dim: int,\n    num_episodes: int,\n    episode_length: int,\n    capacity: int,\n    frame_callback: FrameCallback | None = None,\n    phase_callback: PhaseCallback | None = None,\n    frame_fps: int = 8,\n) -> ReplayBuffer | None:\n    try:\n        import gymnasium as gym\n    except Exception as exc:  # pragma: no cover - depends on optional extras\n        warnings.warn(f"MuJoCo gym collection is unavailable: {exc}")\n        return None\n\n    try:\n        env = gym.make(gym_env, render_mode="rgb_array")\n    except Exception as exc:  # pragma: no cover - depends on optional extras\n        warnings.warn(f"Failed to create MuJoCo env \'{gym_env}\': {exc}")\n        return None\n\n    _emit_phase(phase_callback, "collecting")\n    obs_dim = int(obs_shape[0])\n    buffer = ReplayBuffer(capacity=capacity, obs_shape=(obs_dim,), action_dim=action_dim)\n    min_frame_interval = 1.0 / max(1, int(frame_fps))\n    last_frame_time = 0.0\n\n    for ep_idx in range(num_episodes):\n        obs, _ = env.reset()\n        ep_obs: list[np.ndarray] = []\n        ep_actions: list[np.ndarray] = []\n        ep_rewards: list[float] = []\n        ep_dones: list[float] = []\n        done = False\n        steps = 0\n\n        while not done and steps < episode_length:\n            action = env.action_space.sample()\n            next_obs, reward, terminated, truncated, _ = env.step(action)\n            done = bool(terminated or truncated)\n            ep_obs.append(_fit_vector(obs, obs_dim))\n            ep_actions.append(_fit_vector(action, action_dim))\n            ep_rewards.append(float(reward))\n            ep_dones.append(float(done))\n            now = time.monotonic()\n            if now - last_frame_time >= min_frame_interval or done:\n                frame = env.render()\n                _emit_frame(\n                    frame_callback,\n                    frame,\n                    ep_idx + 1,\n                    steps + 1,\n                    float(reward),\n                    done,\n                )\n                last_frame_time = now\n            obs = next_obs\n            steps += 1\n\n        if ep_obs:\n            buffer.add_episode(\n                obs=np.asarray(ep_obs, dtype=np.float32),\n                actions=np.asarray(ep_actions, dtype=np.float32),\n                rewards=np.asarray(ep_rewards, dtype=np.float32),\n                dones=np.asarray(ep_dones, dtype=np.float32),\n            )\n    env.close()\n    return buffer if len(buffer) > 0 else None\n\n\ndef _resolve_data_settings(model_config: Any) -> dict[str, Any]:\n    cfg = _load_project_config()\n    architecture = cfg.get("architecture", {})\n    data_cfg = cfg.get("data", {})\n    gameplay_cfg = cfg.get("gameplay", {})\n    if not isinstance(gameplay_cfg, dict):\n        gameplay_cfg = {}\n    online_cfg = cfg.get("online_collection", {})\n    if not isinstance(online_cfg, dict):\n        online_cfg = {}\n\n    default_obs = getattr(model_config, "obs_shape", (3, 64, 64))\n    default_action_dim = int(getattr(model_config, "action_dim", 6))\n    obs_shape = _parse_obs_shape(architecture.get("obs_shape", default_obs))\n    action_dim = int(architecture.get("action_dim", default_action_dim))\n\n    source = str(data_cfg.get("source", "random")).strip().lower()\n    num_episodes = max(1, int(data_cfg.get("num_episodes", 100)))\n    episode_length = max(2, int(data_cfg.get("episode_length", 100)))\n    capacity = max(\n        int(data_cfg.get("buffer_capacity", 10000)),\n        num_episodes * episode_length,\n    )\n    gym_env = str(data_cfg.get("gym_env", "")).strip()\n    environment = str(cfg.get("environment", "custom")).strip().lower()\n\n    model_type = str(cfg.get("model_type", "")).strip().lower()\n    online_default = environment == "atari" and model_type.startswith("dreamer")\n\n    return {\n        "obs_shape": obs_shape,\n        "action_dim": action_dim,\n        "source": source,\n        "num_episodes": num_episodes,\n        "episode_length": episode_length,\n        "capacity": capacity,\n        "gym_env": gym_env,\n        "environment": environment,\n        "gameplay_enabled": bool(gameplay_cfg.get("enabled", True)),\n        "gameplay_fps": max(1, int(gameplay_cfg.get("fps", 8))),\n        "online_enabled": bool(online_cfg.get("enabled", online_default)),\n        "warmup_transitions": max(32, int(online_cfg.get("warmup_transitions", 512))),\n        "collect_steps_per_update": max(1, int(online_cfg.get("collect_steps_per_update", 64))),\n        "max_episode_steps": max(8, int(online_cfg.get("max_episode_steps", episode_length))),\n    }\n\n\ndef get_demo_buffer(\n    model_config: Any,\n    frame_callback: FrameCallback | None = None,\n    phase_callback: PhaseCallback | None = None,\n) -> ReplayBuffer:\n    """Return a replay buffer that works out-of-the-box."""\n    settings = _resolve_data_settings(model_config)\n\n    source = settings["source"]\n    obs_shape = settings["obs_shape"]\n    action_dim = settings["action_dim"]\n    num_episodes = settings["num_episodes"]\n    episode_length = settings["episode_length"]\n    capacity = settings["capacity"]\n    gym_env = settings["gym_env"]\n    environment = settings["environment"]\n    gameplay_enabled = settings["gameplay_enabled"]\n    gameplay_fps = settings["gameplay_fps"]\n\n    atari_frame_callback = frame_callback if gameplay_enabled else None\n\n    if source == "gym":\n        buffer: ReplayBuffer | None\n        if environment == "atari":\n            buffer = _collect_atari_buffer(\n                gym_env=gym_env or "ALE/Breakout-v5",\n                obs_shape=obs_shape,\n                action_dim=action_dim,\n                num_episodes=num_episodes,\n                episode_length=episode_length,\n                capacity=capacity,\n                frame_callback=atari_frame_callback,\n                phase_callback=phase_callback,\n                frame_fps=gameplay_fps,\n            )\n        elif environment == "mujoco":\n            mujoco_frame_callback = frame_callback if gameplay_enabled else None\n            buffer = _collect_mujoco_buffer(\n                gym_env=gym_env or "HalfCheetah-v5",\n                obs_shape=obs_shape,\n                action_dim=action_dim,\n                num_episodes=num_episodes,\n                episode_length=episode_length,\n                capacity=capacity,\n                frame_callback=mujoco_frame_callback,\n                phase_callback=phase_callback,\n                frame_fps=gameplay_fps,\n            )\n        elif len(obs_shape) >= 3:\n            buffer = _collect_atari_buffer(\n                gym_env=gym_env or "ALE/Breakout-v5",\n                obs_shape=obs_shape,\n                action_dim=action_dim,\n                num_episodes=num_episodes,\n                episode_length=episode_length,\n                capacity=capacity,\n                frame_callback=atari_frame_callback,\n                phase_callback=phase_callback,\n                frame_fps=gameplay_fps,\n            )\n        else:\n            mujoco_frame_callback = frame_callback if gameplay_enabled else None\n            buffer = _collect_mujoco_buffer(\n                gym_env=gym_env or "HalfCheetah-v5",\n                obs_shape=obs_shape,\n                action_dim=action_dim,\n                num_episodes=num_episodes,\n                episode_length=episode_length,\n                capacity=capacity,\n                frame_callback=mujoco_frame_callback,\n                phase_callback=phase_callback,\n                frame_fps=gameplay_fps,\n            )\n        if buffer is not None:\n            return buffer\n        if gameplay_enabled:\n            _emit_phase(\n                phase_callback,\n                "unavailable",\n                "Gym collection failed. Falling back to random replay buffer.",\n            )\n        warnings.warn("Falling back to random buffer because gym collection failed.")\n    elif gameplay_enabled:\n        _emit_phase(\n            phase_callback,\n            "unavailable",\n            "Set data.source=\'gym\' to enable live gameplay stream.",\n        )\n\n    return create_random_buffer(\n        capacity=capacity,\n        obs_shape=obs_shape,\n        action_dim=action_dim,\n        num_episodes=num_episodes,\n        episode_length=episode_length,\n        seed=42,\n    )\n\n\ndef build_training_data(\n    model_config: Any,\n    frame_callback: FrameCallback | None = None,\n    phase_callback: PhaseCallback | None = None,\n) -> tuple[Any, CleanupFn, str]:\n    """Build either online provider or offline replay buffer for Trainer.train()."""\n    settings = _resolve_data_settings(model_config)\n\n    source = settings["source"]\n    environment = settings["environment"]\n    gameplay_enabled = settings["gameplay_enabled"]\n    online_enabled = settings["online_enabled"]\n\n    if (\n        source == "gym"\n        and online_enabled\n        and (environment == "atari" or len(settings["obs_shape"]) >= 3)\n    ):\n        if gameplay_enabled:\n            _emit_phase(phase_callback, "collecting")\n        try:\n            provider = OnlineAtariBatchProvider(\n                gym_env=settings["gym_env"] or "ALE/Breakout-v5",\n                obs_shape=settings["obs_shape"],\n                action_dim=settings["action_dim"],\n                capacity=settings["capacity"],\n                warmup_transitions=settings["warmup_transitions"],\n                collect_steps_per_update=settings["collect_steps_per_update"],\n                max_episode_steps=settings["max_episode_steps"],\n                frame_callback=frame_callback if gameplay_enabled else None,\n                phase_callback=phase_callback,\n                frame_fps=settings["gameplay_fps"],\n            )\n            return provider, provider.close, "online"\n        except Exception as exc:\n            warnings.warn(f"Online Atari collection is unavailable: {exc}")\n            _emit_phase(phase_callback, "unavailable", str(exc))\n\n    if source == "gym" and online_enabled and environment == "mujoco":\n        if gameplay_enabled:\n            _emit_phase(phase_callback, "collecting")\n        try:\n            provider = OnlineMujocoBatchProvider(\n                gym_env=settings["gym_env"] or "HalfCheetah-v5",\n                obs_shape=settings["obs_shape"],\n                action_dim=settings["action_dim"],\n                capacity=settings["capacity"],\n                warmup_transitions=settings["warmup_transitions"],\n                collect_steps_per_update=settings["collect_steps_per_update"],\n                max_episode_steps=settings["max_episode_steps"],\n                frame_callback=frame_callback if gameplay_enabled else None,\n                phase_callback=phase_callback,\n                frame_fps=settings["gameplay_fps"],\n            )\n            return provider, provider.close, "online"\n        except Exception as exc:\n            warnings.warn(f"Online MuJoCo collection is unavailable: {exc}")\n            _emit_phase(phase_callback, "unavailable", str(exc))\n\n    buffer = get_demo_buffer(\n        model_config,\n        frame_callback=frame_callback,\n        phase_callback=phase_callback,\n    )\n\n    mode = "offline" if source == "gym" else "random"\n    return buffer, (lambda: None), mode\n'
